model_param:
    batch_size: 512
    vocab_size: 800000
    num_layers: 2
    layer_size: 19968
    projection: 2048
    seq_len: 20
    num_gates: 4
    num_non_linear: 5
    num_add: 8 

sw_param:
    kernel_launch_overhead: 37e-6
    precision: 4

tech_param:
    core:
        nominal_energy_per_mcu: 1e-11
        nominal_flop_rate_per_mcu: 2
        nominal_area_per_mcu: 1 
        nominal_frequency: 1e12
        nominal_voltage: 1 
        operating_frequency: 5 
        operating_voltage: 5 
        operating_area_per_mcu: 4
    DRAM:
        energy_per_bit: 6e-12
        area_per_bit: 
        latency: 1e-6 
    L2:
        energy_per_bit: 5e-15
        area_per_bit: 
    shared_mem:
        energy_per_bit: 1e-15
        area_per_bit:
    network:
        topology: ring
        onchip_latency: 16e-7 
        onchip_energy_per_bit: 10e-12
        offchip_latency: 0
        offchip_energy_per_bit: 0

area_param:
    budget: 600
    core: 0.3
    DRAM: 0.2
    L2: 0.3
    shared_mem: 0.1
    IB: 0.1

power_breakdown:
    TDP: 300
    core: 0.625
    DRAM: 0.155
    L2: 0.05
    shared_mem: 0.028
    IB: 0.2

scheduling_param:
    auto: False 
    dp: 1
    lp: 1
    hlp: 1 #FIXME: should not be here, should be calculated based on hidden_dim1 and dim2
    kp_hidden_dim1: 1
    kp_hidden_dim2: 1
    kp_softmax_dim1: 1
    kp_softmax_dim2: 1
    kp_embedding_dim1: 1
    kp_embedding_dim2: 1
    kp_projection_dim1: 1
    kp_projection_dim2: 1
    kp_hidden_type: -1
    kp_softmax_type: -1
    kp_embedding_type: -1
    kp_projection_type: -1
